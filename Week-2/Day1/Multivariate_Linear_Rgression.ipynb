{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ipympl"
      ],
      "metadata": {
        "id": "7T5HvBDJNrgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/Atharva-Malode/ML-Bootcamp.git\""
      ],
      "metadata": {
        "id": "ti32kx6yVT_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/ML-Bootcamp/Week-2/Day1/plots_week2.py\" \"/content/\""
      ],
      "metadata": {
        "id": "xt1onk7IVni1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q5H4zYHeMcBz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "%matplotlib widget\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import copy\n",
        "from matplotlib import animation\n",
        "from plots_week2 import data_visual_2D, plot_3d_graph, cost_vs_iteration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random dataset for each feature\n",
        "m = 100  # Number of examples\n",
        "study_hours = np.random.randint(1, 8, size=m)  # Random study hours ranging from 1 to 7\n",
        "math_score = np.random.randint(40, 100, size=m)  # Random math scores ranging from 40 to 99\n",
        "science_score = np.random.randint(50, 100, size=m)  # Random science scores ranging from 50 to 99\n",
        "english_score = np.random.randint(30, 90, size=m)  # Random English scores ranging from 30 to 89\n",
        "attendance_percentage = np.random.randint(70, 100, size=m)  # Random attendance percentages ranging from 70 to 99\n",
        "\n",
        "# Combine the features into a single dataset\n",
        "X = np.column_stack((study_hours, math_score, science_score, english_score, attendance_percentage))\n",
        "\n",
        "# Generate random target variable (student rank)\n",
        "y = np.random.randint(1, 6, size=m)  # Random student ranks ranging from 1 to 5\n",
        "\n",
        "# Provide explanations for each feature\n",
        "feature_explanations = {\n",
        "    'study_hours': study_hours,\n",
        "    'math_score': math_score,\n",
        "    'science_score': science_score,\n",
        "    'english_score': english_score,\n",
        "    'attendance_percentage': attendance_percentage,\n",
        "    \"Student Rank\": y,\n",
        "}\n",
        "# Create a DataFrame using the dictionary\n",
        "dataset = pd.DataFrame(feature_explanations)\n",
        "\n",
        "# Initialize initial values for w and b\n",
        "n_features = X.shape[1]\n",
        "w_initial = np.zeros((n_features,))\n",
        "b_initial = 0.0\n",
        "\n",
        "alpha = 9e-7  # Learning rate\n",
        "num_iters = 100  # Number of iterations\n",
        "dataset"
      ],
      "metadata": {
        "id": "YqMxdYRMR-iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_visual_2D(n_features, feature_explanations, X,y)"
      ],
      "metadata": {
        "id": "-NThphIAL_m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_3d_graph(X,y)"
      ],
      "metadata": {
        "id": "Ki3VwLGabJ-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b): \n",
        "    \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):                                \n",
        "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
        "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
        "    cost = cost / (2 * m)                      #scalar    \n",
        "    return cost"
      ],
      "metadata": {
        "id": "BNOVX4SiOHu9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):                             \n",
        "        err = (np.dot(X[i], w) + b) - y[i]   \n",
        "        for j in range(n):                         \n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
        "        dj_db = dj_db + err                        \n",
        "    dj_dw = dj_dw / m                                \n",
        "    dj_db = dj_db / m                                \n",
        "        \n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "PcBmmCouOPJb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
        "    num_iters gradient steps with learning rate alpha\n",
        "    \n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters  \n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "      \n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters \n",
        "      b (scalar)       : Updated value of parameter \n",
        "      \"\"\"\n",
        "    \n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               ##None\n",
        "        b = b - alpha * dj_db               ##None\n",
        "      \n",
        "        # Save cost J at each iteration\n",
        "        if i<100:      # prevent resource exhaustion \n",
        "            J_history.append( cost_function(X, y, w, b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
        "        \n",
        "    return w, b, J_history #return final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "6bMVwpRVOP9p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform gradient descent\n",
        "w_final, b_final, J_history = gradient_descent(X, y, w_initial, b_initial, compute_cost, compute_gradient, alpha, num_iters)"
      ],
      "metadata": {
        "id": "xCT45LngnbVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_vs_iteration(J_history, num_iters)"
      ],
      "metadata": {
        "id": "cvR5ptgRNTH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close()\n",
        "# Generate some random data for demonstration\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2)\n",
        "y = 2 + 3*X[:, 0] - 4*X[:, 1] + np.random.randn(100)\n",
        "\n",
        "# Perform gradient descent\n",
        "lr = 0.01  # Learning rate\n",
        "num_iters = 100\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(theta0, theta1, theta2):\n",
        "    y_pred = theta0 + theta1*X[:, 0] + theta2*X[:, 1]\n",
        "    return np.mean((y_pred - y)**2)\n",
        "\n",
        "# Initialize parameters and history\n",
        "theta0_hist, theta1_hist, theta2_hist, loss_hist = [], [], [], []\n",
        "theta0 = 0.0\n",
        "theta1 = 0.0\n",
        "theta2 = 0.0\n",
        "\n",
        "for i in range(num_iters):\n",
        "    # Calculate gradients\n",
        "    gradients = [\n",
        "        np.mean(theta0 + theta1*X[:, 0] + theta2*X[:, 1] - y),\n",
        "        np.mean((theta0 + theta1*X[:, 0] + theta2*X[:, 1] - y) * X[:, 0]),\n",
        "        np.mean((theta0 + theta1*X[:, 0] + theta2*X[:, 1] - y) * X[:, 1])\n",
        "    ]\n",
        "\n",
        "    # Update parameters\n",
        "    theta0 -= lr * gradients[0]\n",
        "    theta1 -= lr * gradients[1]\n",
        "    theta2 -= lr * gradients[2]\n",
        "\n",
        "    # Save parameter values and loss for animation\n",
        "    theta0_hist.append(theta0)\n",
        "    theta1_hist.append(theta1)\n",
        "    theta2_hist.append(theta2)\n",
        "    loss_hist.append(loss_function(theta0, theta1, theta2))\n",
        "\n",
        "# Set up the figure and axes\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Create grid of parameter values\n",
        "theta0_vals = np.linspace(-10, 10, 100)\n",
        "theta1_vals = np.linspace(-10, 10, 100)\n",
        "theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)\n",
        "loss_vals = np.zeros_like(theta0_vals)\n",
        "\n",
        "for i in range(theta0_vals.shape[0]):\n",
        "    for j in range(theta0_vals.shape[1]):\n",
        "        loss_vals[i, j] = loss_function(theta0_vals[i, j], theta1_vals[i, j], 0)  # Fix theta2 = 0 for visualization\n",
        "\n",
        "# Plot the loss function contour\n",
        "contour = ax.contour(theta0_vals, theta1_vals, loss_vals, levels=20, cmap='viridis')\n",
        "\n",
        "# Plot the history of parameter values and steps\n",
        "line, = ax.plot([], [], 'r', marker='o')\n",
        "\n",
        "# Function to update the animation\n",
        "def update_animation(frame):\n",
        "    line.set_data(theta0_hist[:frame], theta1_hist[:frame])\n",
        "    contour.collections[0].set_alpha(float(frame) / num_iters)\n",
        "    return line, contour\n",
        "\n",
        "# Create animation\n",
        "animation = animation.FuncAnimation(fig, update_animation, frames=num_iters, interval=100, blit=True)\n",
        "\n",
        "# Set plot labels\n",
        "ax.set_xlabel('Theta 0')\n",
        "ax.set_ylabel('Theta 1')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LzF1WrMVq-E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c74rTrczTAYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}