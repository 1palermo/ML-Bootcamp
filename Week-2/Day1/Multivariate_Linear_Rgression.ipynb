{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T5HvBDJNrgD"
      },
      "outputs": [],
      "source": [
        "!pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti32kx6yVT_j"
      },
      "outputs": [],
      "source": [
        "!git clone \"https://github.com/Atharva-Malode/ML-Bootcamp.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xt1onk7IVni1"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/ML-Bootcamp/Week-2/Day1/plots_week2.py\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q5H4zYHeMcBz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "%matplotlib widget\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import copy\n",
        "from matplotlib import animation\n",
        "from plots_week2 import data_visual_2D, plot_3d_graph, cost_vs_iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqMxdYRMR-iw"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random dataset for each feature\n",
        "m = 100  # Number of examples\n",
        "study_hours = np.random.randint(1, 8, size=m)  # Random study hours ranging from 1 to 7\n",
        "math_score = np.random.randint(40, 100, size=m)  # Random math scores ranging from 40 to 99\n",
        "science_score = np.random.randint(50, 100, size=m)  # Random science scores ranging from 50 to 99\n",
        "english_score = np.random.randint(30, 90, size=m)  # Random English scores ranging from 30 to 89\n",
        "attendance_percentage = np.random.randint(70, 100, size=m)  # Random attendance percentages ranging from 70 to 99\n",
        "\n",
        "# Combine the features into a single dataset\n",
        "X = np.column_stack((study_hours, math_score, science_score, english_score, attendance_percentage))\n",
        "\n",
        "# Generate random target variable (student rank)\n",
        "y = np.random.randint(1, 6, size=m)  # Random student ranks ranging from 1 to 5\n",
        "\n",
        "# Provide explanations for each feature\n",
        "feature_explanations = {\n",
        "    'study_hours': study_hours,\n",
        "    'math_score': math_score,\n",
        "    'science_score': science_score,\n",
        "    'english_score': english_score,\n",
        "    'attendance_percentage': attendance_percentage,\n",
        "    \"Student Rank\": y,\n",
        "}\n",
        "# Create a DataFrame using the dictionary\n",
        "dataset = pd.DataFrame(feature_explanations)\n",
        "\n",
        "# Initialize initial values for w and b\n",
        "n_features = X.shape[1]\n",
        "w_initial = np.zeros((n_features,))\n",
        "b_initial = 0.0\n",
        "\n",
        "alpha = 9e-7  # Learning rate\n",
        "num_iters = 100  # Number of iterations\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NThphIAL_m9"
      },
      "outputs": [],
      "source": [
        "data_visual_2D(n_features, feature_explanations, X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki3VwLGabJ-r"
      },
      "outputs": [],
      "source": [
        "plot_3d_graph(X,y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1 Compute Cost With Multiple Variables\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cost(X, y, w, b): \n",
        "    \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):                                \n",
        "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
        "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
        "    cost = cost / (2 * m)                      #scalar    \n",
        "    return cost"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Gradient Descent With Multiple Variables"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Calculating the error\n",
        "The Error for every example is calculated as:\n",
        "\n",
        "$$\\text{error}^{(i)} = f_w(x^{(i)}) - y^{(i)}$$\n",
        "\n",
        "where,\n",
        "* $f_w(x^{(i)}) = w_0 + w_1x_1^{(i)} + w_2x_2^{(i)} + \\ldots + w_nx_n^{(i)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_error(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the error for linear regression\n",
        "    Args:\n",
        "      x (ndarray (n,)): Data for a single example with n features\n",
        "      y (scalar)       : target value\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      error (scalar): The error between predicted value and target value.\n",
        "    \"\"\"\n",
        "    error = (np.dot(x, w) + b) - y\n",
        "    return error"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Calculating Gradient\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\text{error}^{(i)})x_{j}^{(i)} \\tag{3}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\text{error}^{(i)}) \\tag{4}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PcBmmCouOPJb"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        error = compute_error(X[i], y[i], w, b)\n",
        "        \n",
        "        for j in range(n):\n",
        "            dj_dw[j] += error * X[i, j]\n",
        "        dj_db += error\n",
        "\n",
        "    dj_dw /= m\n",
        "    dj_db /= m\n",
        "        \n",
        "    return dj_db, dj_dw"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6bMVwpRVOP9p"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
        "    num_iters gradient steps with learning rate alpha\n",
        "    \n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters  \n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "      \n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters \n",
        "      b (scalar)       : Updated value of parameter \n",
        "      \"\"\"\n",
        "    \n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               ##None\n",
        "        b = b - alpha * dj_db               ##None\n",
        "      \n",
        "        # Save cost J at each iteration\n",
        "        if i<100:      # prevent resource exhaustion \n",
        "            J_history.append( cost_function(X, y, w, b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
        "        \n",
        "    return w, b, J_history #return final w,b and J history for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCT45LngnbVg"
      },
      "outputs": [],
      "source": [
        "# Perform gradient descent\n",
        "w_final, b_final, J_history = gradient_descent(X, y, w_initial, b_initial, compute_cost, compute_gradient, alpha, num_iters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvR5ptgRNTH8"
      },
      "outputs": [],
      "source": [
        "cost_vs_iteration(J_history, num_iters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzF1WrMVq-E3"
      },
      "outputs": [],
      "source": [
        "plt.close()\n",
        "# Generate some random data for demonstration\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2)\n",
        "y = 2 + 3*X[:, 0] - 4*X[:, 1] + np.random.randn(100)\n",
        "\n",
        "# Perform gradient descent\n",
        "lr = 0.01  # Learning rate\n",
        "num_iters = 100\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(theta0, theta1, theta2):\n",
        "    y_pred = theta0 + theta1*X[:, 0] + theta2*X[:, 1]\n",
        "    return np.mean((y_pred - y)**2)\n",
        "\n",
        "# Initialize parameters and history\n",
        "theta0_hist, theta1_hist, theta2_hist, loss_hist = [], [], [], []\n",
        "theta0 = 0.0\n",
        "theta1 = 0.0\n",
        "theta2 = 0.0\n",
        "\n",
        "for i in range(num_iters):\n",
        "    # Calculate gradients\n",
        "    gradients = [\n",
        "        np.mean(theta0 + theta1*X[:, 0] + theta2*X[:, 1] - y),\n",
        "        np.mean((theta0 + theta1*X[:, 0] + theta2*X[:, 1] - y) * X[:, 0]),\n",
        "        np.mean((theta0 + theta1*X[:, 0] + theta2*X[:, 1] - y) * X[:, 1])\n",
        "    ]\n",
        "\n",
        "    # Update parameters\n",
        "    theta0 -= lr * gradients[0]\n",
        "    theta1 -= lr * gradients[1]\n",
        "    theta2 -= lr * gradients[2]\n",
        "\n",
        "    # Save parameter values and loss for animation\n",
        "    theta0_hist.append(theta0)\n",
        "    theta1_hist.append(theta1)\n",
        "    theta2_hist.append(theta2)\n",
        "    loss_hist.append(loss_function(theta0, theta1, theta2))\n",
        "\n",
        "# Set up the figure and axes\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Create grid of parameter values\n",
        "theta0_vals = np.linspace(-10, 10, 100)\n",
        "theta1_vals = np.linspace(-10, 10, 100)\n",
        "theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)\n",
        "loss_vals = np.zeros_like(theta0_vals)\n",
        "\n",
        "for i in range(theta0_vals.shape[0]):\n",
        "    for j in range(theta0_vals.shape[1]):\n",
        "        loss_vals[i, j] = loss_function(theta0_vals[i, j], theta1_vals[i, j], 0)  # Fix theta2 = 0 for visualization\n",
        "\n",
        "# Plot the loss function contour\n",
        "contour = ax.contour(theta0_vals, theta1_vals, loss_vals, levels=20, cmap='viridis')\n",
        "\n",
        "# Plot the history of parameter values and steps\n",
        "line, = ax.plot([], [], 'r', marker='o')\n",
        "\n",
        "# Function to update the animation\n",
        "def update_animation(frame):\n",
        "    line.set_data(theta0_hist[:frame], theta1_hist[:frame])\n",
        "    contour.collections[0].set_alpha(float(frame) / num_iters)\n",
        "    return line, contour\n",
        "\n",
        "# Create animation\n",
        "animation = animation.FuncAnimation(fig, update_animation, frames=num_iters, interval=100, blit=True)\n",
        "\n",
        "# Set plot labels\n",
        "ax.set_xlabel('Theta 0')\n",
        "ax.set_ylabel('Theta 1')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
